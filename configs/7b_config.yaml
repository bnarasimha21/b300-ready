# 7B Model Training Configuration for 8x B300
# Ready to use with NeMo or custom training script

model:
  name: "llama-7b"
  hidden_size: 4096
  intermediate_size: 11008
  num_hidden_layers: 32
  num_attention_heads: 32
  num_key_value_heads: 8  # GQA
  vocab_size: 32000
  max_position_embeddings: 4096
  rope_theta: 10000.0

training:
  # Batch sizes
  micro_batch_size: 4
  global_batch_size: 256
  gradient_accumulation_steps: 8
  
  # Parallelism (8x B300)
  tensor_parallel_size: 2
  pipeline_parallel_size: 1
  data_parallel_size: 4
  
  # Precision
  precision: "bf16-mixed"
  fp8: true
  
  # Optimization
  learning_rate: 3.0e-4
  min_learning_rate: 3.0e-5
  warmup_steps: 2000
  max_steps: 100000
  weight_decay: 0.1
  grad_clip: 1.0
  
  # Schedule
  lr_scheduler: "cosine"
  
  # Checkpointing
  save_interval: 1000
  eval_interval: 500

data:
  seq_length: 4096
  train_data_path: "data/fineweb"
  
hardware:
  num_gpus: 8
  gpu_type: "B300"
  memory_per_gpu: 192  # GB
